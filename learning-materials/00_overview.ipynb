{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db330331",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ffffff; color: #000000; padding: 10px;\">\n",
    "<img src=\"../media/kisz_logo.png\" width=\"192\" height=\"69\"> \n",
    "<h1> Video Search\n",
    "<h2> Finding Locations in Audio and Video with Automatic Speech Recognition and Semantic Search\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f90f35",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<div style=\"background-color: #f6a800; color: #ffffff; padding: 10px;\">\n",
    "    <h2> Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a56275",
   "metadata": {},
   "source": [
    "The learning materials introduce basic concepts underlying OpenAI's Whisper model. The focus of this notebook is to explain how the training data is processed in the course of the training of Whisper. The training data can be divided into input data (i.e., audios) and output data (i.e., textual transcriptions), whereby the first notebook focuses the processing of the audio and the second and third notebooks focus on the processing of text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ba2b7d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<div style=\"background-color: #dd6108; color: #ffffff; padding: 10px;\">\n",
    "<h3><a href=\"01_digital_signal_processing.ipynb\" style=\"color: #ffffff;\">1. Introduction to Digital Signal Processing</a></h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb869a4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- Introduction to digital audio signals and their properties\n",
    "- Explanation of sampling, quantisation, and the frequency domain\n",
    "- Visualisation and interpretation of waveforms and spectrograms\n",
    "- Understanding how frequency content shapes audio perception"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42279ff",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<div style=\"background-color: #dd6108; color: #ffffff; padding: 10px;\">\n",
    "<h3><a href=\"02_byte_pair_encoding.ipynb\" style=\"color: #ffffff;\">2. Tokenisation and Byte Pair Encoding</a></h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7927b63",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- Overview of tokenisation and its importance in language models\n",
    "- Introduction to special tokens and vocabulary construction\n",
    "- Explanation and step-by-step example of the Byte Pair Encoding (BPE) algorithm\n",
    "- Implementation and demonstration of a simple BPE tokeniser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e49b19",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<div style=\"background-color: #dd6108; color: #ffffff; padding: 10px;\">\n",
    "<h3><a href=\"03_embeddings.ipynb\" style=\"color: #ffffff;\">3. Creating Embeddings</a></h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bf7ee8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- Explanation of word and token embeddings for neural networks\n",
    "- How embeddings are used to represent text for LLMs\n",
    "- Introduction to embedding layers and positional encodings\n",
    "- Preparing tokenised data for model training using PyTorch"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
