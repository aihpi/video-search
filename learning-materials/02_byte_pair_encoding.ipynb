{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83e5ef82",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ffffff; color: #000000; padding: 10px;\">\n",
    "<img src=\"../media/kisz_logo.png\" width=\"192\" height=\"69\"> \n",
    "<h1> Video Search\n",
    "<h2> Finding Locations in Audio and Video with Automatic Speech Recognition and Semantic Search\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aa40e3-5109-433f-9153-f5770531fe94",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f6a800; color: #ffffff; padding: 10px;\">\n",
    "    <h2> Part 2 - Byte Pair Encoding\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1641f131",
   "metadata": {},
   "source": [
    "When working with language models like OpenAI's Whisper or large language models (LLMs), we need a way to represent text in a format that neural networks can understand. Word embeddings provide this bridge by converting discrete language units (e.g., words, punctuation) into continuous vectors of real numbers (e.g., `[-0.342, 0.234, 0.633, 0.45]`). These vectors capture semantic and syntactic information, enabling models to recognise patterns, similarities, and relationships in language data. Without embeddings, models would struggle to process raw text efficiently or generalise across linguistic contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddbb984-8d23-40c5-bbfa-c3c379e7eec3",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #dd6108; color: #ffffff; padding: 10px;\">\n",
    "<h3>1. Tokenisation - Breaking down Text into small Units\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c90731-7dc9-4cd3-8c4a-488e33b48e80",
   "metadata": {},
   "source": [
    "In order to create embeddings, we need to tokenise the text that we will use for model training. Tokenisation is the process of breaking down continuous texts into smaller units such as words and punctuation characters. Each unique token will get its own ID. In the following, tokenisation is illustrated in the context of the novel *Moby Dick*, which can be downloaded from the *Gutenberg Corpus*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40f9d9b1-6d32-485a-825a-a95392a86d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meet a whale - ship on the ocean without being struck by her near appearance . The vessel under shor\n",
      "Total number of characters: 1259862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/hanmul/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# get text from novel Moby Dick\n",
    "moby_dick = gutenberg.words('melville-moby_dick.txt')\n",
    "moby_dick = ' '.join(moby_dick)\n",
    "\n",
    "# print a few words to get a feeling for the data\n",
    "print(moby_dick[20000:20100])\n",
    "print(\"Total number of characters:\", len(moby_dick))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97811f16",
   "metadata": {},
   "source": [
    "For creating tokens, we first split the text into words and punctuation characters. Each of these *units* will be the basis for a token. These units will not be our final tokens, because some units will be further decomposed, as is detailled out below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bb193eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Moby', 'Dick', 'by', 'Herman', 'Melville', '1851', ']', 'ETYMOLOGY', '.', '(', 'Supplied', 'by', 'a', 'Late', 'Consumptive', 'Usher', 'to', 'a', 'Grammar', 'School', ')', 'The', 'pale', 'Usher', '--', 'threadbare', 'in', 'coat', ',']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# split the text into tokens\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', moby_dick)\n",
    "\n",
    "# remove empty tokens\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "# investigate\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5204973-f414-4c0d-87b0-cfec1f06e6ff",
   "metadata": {},
   "source": [
    "Next, we convert the text tokens into token IDs, representing our vocabulary. The vocabulary consists of each unique token, sorted alphabetically. Each unique token is then mapped onto its ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fdf0533-5ab6-42a5-83fa-a3b045de6396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 19243 unique tokens.\n",
      "('!', 0)\n",
      "('\"', 1)\n",
      "('$', 2)\n",
      "('&', 3)\n",
      "(\"'\", 4)\n",
      "('(', 5)\n",
      "(')', 6)\n",
      "('*', 7)\n",
      "(',', 8)\n",
      "('-', 9)\n",
      "('--', 10)\n"
     ]
    }
   ],
   "source": [
    "# create the vocabulary\n",
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(f'vocabulary size: {vocab_size} unique tokens.')\n",
    "\n",
    "# print the first items of the vocabulary\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b821ef8-4d53-43b6-a2b2-aef808c343c7",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #dd6108; color: #ffffff; padding: 10px;\">\n",
    "<h3>2. Special Context Tokens\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d709d57-2486-4152-b7f9-d3e4bd8634cd",
   "metadata": {},
   "source": [
    "Often, special tokens are added to the vocabulary for different reasons.\n",
    "- `<|UNK|>` (unknown word) denotes out-of-vocabulary words\n",
    "- `<|endoftext|>` or `<|EOS|>` (end of sequence) are used, among others, when multiple texts such as newspaper articles are concatenated\n",
    "- `<|beginningoftext|>` or `<|BOS|>` denot beginnings analogous to `<|endoftext|>` and `<|EOS|>`\n",
    "- `<|PAD|>` (padding) is used when training LLMs with batch sizes greater than 1 (i.e., inputs with different length are 'padded' to the length of the longest input; e.g., `['Never', 'mind', '<|PAD|>', '<|PAD|>', '<|PAD|>']` has the same length as `['Actions', 'speak', 'louder', 'than', 'words']`\n",
    "\n",
    "For illustrating the concept of special tokens, let us look at an example, where we encounter an unknown word, that is, a word that is not included in our vocabulary. Encoding this text does not work, because the word has no ID associated with it. In the folowing example, the out-of-vocabulary word is \"Hello\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce9df29c-6c5b-43f1-8c1a-c7f7b79db78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('zone', 19239)\n",
      "('zoned', 19240)\n",
      "('zones', 19241)\n",
      "('zoology', 19242)\n",
      "('<|UNK|>', 19243)\n"
     ]
    }
   ],
   "source": [
    "# add <|unk|> to vocabulary\n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|UNK|>\"])\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "\n",
    "# check whether it worked\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee7a1e5-b54f-4ca1-87ef-3d663c4ee1e7",
   "metadata": {},
   "source": [
    "We can now define `encode` and `decode` functions for a simple tokeniser. In the AI domain, encoding refers to the process of transforming a input sequence into tokens, whereas decoding refers to the reverse transformation, that is, tokens to output sequence. In the case of this notebook, input and output sequences are texts. With the `encode` and the `decode` function, we can turn text into IDs and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "948861c5-3f30-4712-a234-725f20d26f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int \n",
    "            else \"<|UNK|>\" for item in preprocessed # add <|UNK|> for unknown tokens\n",
    "        ]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d786a6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this sentence is a test sentence.\n",
      "[5, 3, 2, 1, 4, 3, 0]\n"
     ]
    }
   ],
   "source": [
    "sentence = sorted(set(['this', 'sentence', 'is', 'a', 'test', 'sentence', '.']))\n",
    "vocab_small = {token:integer for integer,token in enumerate(sentence)}\n",
    "\n",
    "tokenizer = SimpleTokenizer(vocab_small)\n",
    "\n",
    "# print test sentence and corresponding token IDs\n",
    "ids = tokenizer.encode('this sentence is a test sentence.')\n",
    "print(tokenizer.decode(ids))\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f24fc78",
   "metadata": {},
   "source": [
    "After sorting alphabetically, the \".\" is the first token, which gets the ID 0. The second token (ID 1) is \"a\". The token \"sentence\", which occurs twice, get the ID 3. Using the toy vocabulary, the relationship between tokens and IDs is straightforward. However, in real texts that are much longer, it is not possible anymore to see this relationship at one glance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "647364ec-7995-4654-9b4a-7607ccf5f1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ship on the ocean without being struck by her near appearance. The vessel under short sail, with look - outs at the mast - heads, eagerly scanning the wide expanse around them, has\n",
      "[15641, 12867, 17275, 12802, 19050, 4821, 16694, 5471, 9992, 12548, 4227, 11, 3296, 18487, 17981, 15683, 15152, 8, 19037, 11657, 9, 12993, 4435, 17275, 11960, 9, 9897, 8, 7843, 15255, 17275, 18965, 8389, 4322, 17280, 8, 9839]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizer(vocab)\n",
    "\n",
    "text = moby_dick[20015:20200]\n",
    "\n",
    "# eprint text and corresponding IDs\n",
    "ids = tokenizer.encode(text)\n",
    "print(tokenizer.decode(ids))\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa728dd1-9d35-4ac7-938f-d411d73083f6",
   "metadata": {},
   "source": [
    "Out-of-vocabulary words are encoded with the ID that represents the special token `<|UNK|>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4133c502-18ac-4412-9f43-01caf4efa3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|UNK|> World\n",
      "[19243, 3640]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizer(vocab)\n",
    "\n",
    "text = \"Hello World\"\n",
    "\n",
    "print(tokenizer.decode(tokenizer.encode(text)))\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4ba34b-170f-4e71-939b-77aabb776f14",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #dd6108; color: #ffffff; padding: 10px;\">\n",
    "<h3>3. Byte Pair Encoding\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21223687",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65f4eba4",
   "metadata": {},
   "source": [
    "TO-DO\n",
    "- Explain motivation\n",
    "- long-tailed word distribution\n",
    "- repetitive patterns in longer words\n",
    "- relationship vocabulary size and training\n",
    "- examples vocabulary size different LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e692aec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2309494c-79cf-4a2d-bc28-a94d602f050e",
   "metadata": {},
   "source": [
    "Whisper uses Byte Pair encoding (BPE) as its tokeniser. BPE breaks down words into smaller subword units or even individual characters in order to handle out-of-vocabulary words instead of using an `<|UNK|>` token. For instance, an out-of-vocabulary word such as \"Uncharacteristically\" may be broken down into the tokens `['Unchar', 'acter', 'isti', 'cally']` or some other subword breakdown, depending on how BPE is implemented.\n",
    "\n",
    "The original BPE tokeniser that OpenAI implemented for training the original GPT models can be found [here](https://github.com/openai/gpt-2/blob/master/src/encoder.py). The BPE algorithm was originally described in 1994: \"[A New Algorithm for Data Compression](http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM)\" by Philip Gage. Most projects use OpenAI's open-source [tiktoken library](https://github.com/openai/tiktoken) due to its computational performance; it allows loading pretrained GPT-2 and GPT-4 tokenisers, for example (the Llama 3 models were trained using the GPT-4 tokeniser). There's also an implementation called [minBPE](https://github.com/karpathy/minbpe) with training support. Additionally, Hugging Face tokenisers are also capable of training and loading various tokenisers; see [this GitHub discussion](https://github.com/rasbt/LLMs-from-scratch/discussions/485) by a reader who trained a BPE tokenizer on the Nepali language for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8a563f",
   "metadata": {},
   "source": [
    "## The Main Idea behind Byte Pair Encoding (BPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0339428f",
   "metadata": {},
   "source": [
    "## Bits and bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c4f887",
   "metadata": {},
   "source": [
    "- Before getting to the BPE algorithm, let's introduce the notion of bytes\n",
    "- Consider converting text into a byte array (BPE stands for \"byte\" pair encoding after all):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bec42460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytearray(b'Hello World')\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello World\"\n",
    "byte_ary = bytearray(text, \"utf-8\")\n",
    "print(byte_ary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46abc48",
   "metadata": {},
   "source": [
    "- When we call `list()` on a `bytearray` object, each byte is treated as an individual element, and the result is a list of integers corresponding to the byte values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "765ab6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72, 101, 108, 108, 111, 32, 87, 111, 114, 108, 100]\n"
     ]
    }
   ],
   "source": [
    "ids = list(byte_ary)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35a1430",
   "metadata": {},
   "source": [
    "- This would be a valid way to convert text into a token ID representation that we need for the embedding layer of an LLM\n",
    "- However, the downside of this approach is that it is creating one ID for each character (that's a lot of IDs for a short text!)\n",
    "- I.e., this means for a 11-character input text, we have to use 11 token IDs as input to the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc9caeb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 11\n",
      "Number of token IDs: 11\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of characters:\", len(text))\n",
    "print(\"Number of token IDs:\", len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb6a9362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15496, 2159]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "gpt2_tokenizer.encode(\"Hello World\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392d9c00",
   "metadata": {},
   "source": [
    "- If you have worked with LLMs before, you may know that the BPE tokenizers have a vocabulary where we have a token ID for whole words or subwords instead of each character\n",
    "- For example, the GPT-2 tokenizer tokenizes the same text (\"Hello World\") into only 2 instead of 11 tokens.\n",
    "- You can double-check this using the interactive [tiktoken app](https://tiktokenizer.vercel.app/?model=gpt2) or the [tiktoken library](https://github.com/openai/tiktoken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b513d66",
   "metadata": {},
   "source": [
    "Since a byte consists of 8 bits, there are 2<sup>8</sup> = 256 possible values that a single byte can represent, ranging from 0 to 255. You can confirm this by executing the code `bytearray(range(0, 257))`, which will warn you that `ValueError: byte must be in range(0, 256)`). A BPE tokenizer usually uses these 256 values as its first 256 single-character tokens; one could visually check this by running the code below. Note that entries 256 and 257 are not single-character values but double-character values (a whitespace + a letter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f4c07a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250: �\n",
      "251: �\n",
      "252: �\n",
      "253: �\n",
      "254: �\n",
      "255: �\n",
      "256:  t\n",
      "257:  a\n",
      "258: he\n",
      "259: in\n"
     ]
    }
   ],
   "source": [
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "for i in range(250, 260):\n",
    "    decoded = gpt2_tokenizer.decode([i])\n",
    "    print(f\"{i}: {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2418f4d",
   "metadata": {},
   "source": [
    "## Building the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a78166",
   "metadata": {},
   "source": [
    "The goal of the BPE tokenization algorithm is to build a vocabulary of commonly occurring subwords like `298: ent` (which can be found in *entangle, entertain, enter, entrance, entity, ...*, for example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "685372b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ent'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_tokenizer.decode([298])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bdd566",
   "metadata": {},
   "source": [
    "... or even complete words like "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc005b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318:  is\n",
      "617:  some\n",
      "1212: This\n",
      "2420:  text\n"
     ]
    }
   ],
   "source": [
    "for i in [318, 617, 1212, 2420]:\n",
    "    decoded = gpt2_tokenizer.decode([i])\n",
    "    print(f\"{i}: {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a946a8",
   "metadata": {},
   "source": [
    "The BPE algorithm was originally described in 1994: \"[A New Algorithm for Data Compression](http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM)\" by Philip Gage. Before we get to the actual code implementation, the form that is used for LLM tokenizers today can be summarized as described in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cdc4f8",
   "metadata": {},
   "source": [
    "## BPE algorithm outline\n",
    "\n",
    "**1. Identify frequent pairs**\n",
    "- In each iteration, scan the text to find the most commonly occurring pair of bytes (or characters)\n",
    "\n",
    "**2. Replace and record**\n",
    "\n",
    "- Replace that pair with a new placeholder ID (one not already in use, e.g., if we start with 0...255, the first placeholder would be 256)\n",
    "- Record this mapping in a lookup table\n",
    "- The size of the lookup table is a hyperparameter, also called \"vocabulary size\" (for GPT-2, that's\n",
    "50,257)\n",
    "\n",
    "**3. Repeat until no gains**\n",
    "\n",
    "- Keep repeating steps 1 and 2, continually merging the most frequent pairs\n",
    "- Stop when no further compression is possible (e.g., no pair occurs more than once)\n",
    "\n",
    "**Decompression (decoding)**\n",
    "\n",
    "- To restore the original text, reverse the process by substituting each ID with its corresponding pair, using the lookup table\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db823131",
   "metadata": {},
   "source": [
    "## BPE algorithm example\n",
    "\n",
    "### Concrete example of the encoding part (steps 1 & 2)\n",
    "\n",
    "Suppose we have the text (training dataset) `the cat in the hat` from which we want to build the vocabulary for a BPE tokenizer\n",
    "\n",
    "**Iteration 1**\n",
    "\n",
    "1. Identify frequent pairs\n",
    "  - In this text, \"th\" appears twice (at the beginning and before the second \"e\")\n",
    "\n",
    "2. Replace and record\n",
    "  - replace \"th\" with a new token ID that is not already in use, e.g., 256\n",
    "  - the new text is: `<256>e cat in <256>e hat`\n",
    "  - the new vocabulary is\n",
    "\n",
    "```\n",
    "  0: ...\n",
    "  ...\n",
    "  256: \"th\"\n",
    "```\n",
    "\n",
    "**Iteration 2**\n",
    "\n",
    "1. **Identify frequent pairs**  \n",
    "   - In the text `<256>e cat in <256>e hat`, the pair `<256>e` appears twice\n",
    "\n",
    "2. **Replace and record**  \n",
    "   - replace `<256>e` with a new token ID that is not already in use, for example, `257`.  \n",
    "   - The new text is:\n",
    "     ```\n",
    "     <257> cat in <257> hat\n",
    "     ```\n",
    "   - The updated vocabulary is:\n",
    "     ```\n",
    "     0: ...\n",
    "     ...\n",
    "     256: \"th\"\n",
    "     257: \"<256>e\"\n",
    "     ```\n",
    "\n",
    "**Iteration 3**\n",
    "\n",
    "1. **Identify frequent pairs**  \n",
    "   - In the text `<257> cat in <257> hat`, the pair `<257> ` appears twice (once at the beginning and once before “hat”).\n",
    "\n",
    "2. **Replace and record**  \n",
    "   - replace `<257> ` with a new token ID that is not already in use, for example, `258`.  \n",
    "   - the new text is:\n",
    "     ```\n",
    "     <258>cat in <258>hat\n",
    "     ```\n",
    "   - The updated vocabulary is:\n",
    "     ```\n",
    "     0: ...\n",
    "     ...\n",
    "     256: \"th\"\n",
    "     257: \"<256>e\"\n",
    "     258: \"<257> \"\n",
    "     ```\n",
    "     \n",
    "- and so forth\n",
    "\n",
    "### Concrete example of the decoding part (step 3)\n",
    "\n",
    "- To restore the original text, we reverse the process by substituting each token ID with its corresponding pair in the reverse order they were introduced\n",
    "- Start with the final compressed text: `<258>cat in <258>hat`\n",
    "-  Substitute `<258>` → `<257> `: `<257> cat in <257> hat`  \n",
    "- Substitute `<257>` → `<256>e`: `<256>e cat in <256>e hat`\n",
    "- Substitute `<256>` → \"th\": `the cat in the hat`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd08f47f",
   "metadata": {},
   "source": [
    "## 2. A simple BPE implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcbe67f",
   "metadata": {},
   "source": [
    "Below is an implementation of this algorithm described above as a Python class that mimics the `tiktoken` Python user interface. Note that the encoding part above describes the original training step via `train()`; however, the `encode()` method works similarly (although it looks a bit more complicated because of the special token handling):\n",
    "\n",
    "1. Split the input text into individual bytes\n",
    "2. Repeatedly find & replace (merge) adjacent tokens (pairs) when they match any pair in the learned BPE merges (from highest to lowest \"rank,\" i.e., in the order they were learned)\n",
    "3. Continue merging until no more merges can be applied\n",
    "4. The final list of token IDs is the encoded output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eac71d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, deque\n",
    "from functools import lru_cache\n",
    "\n",
    "\n",
    "class BPETokenizerSimple:\n",
    "    def __init__(self):\n",
    "        # Maps token_id to token_str (e.g., {11246: \"some\"})\n",
    "        self.vocab = {}\n",
    "        # Maps token_str to token_id (e.g., {\"some\": 11246})\n",
    "        self.inverse_vocab = {}\n",
    "        # Dictionary of BPE merges: {(token_id1, token_id2): merged_token_id}\n",
    "        self.bpe_merges = {}\n",
    "\n",
    "        # For the official OpenAI GPT-2 merges, use a rank dict:\n",
    "        # of form {(string_A, string_B): rank}, where lower rank = higher priority\n",
    "        self.bpe_ranks = {}\n",
    "\n",
    "    def train(self, text, vocab_size, allowed_special={\"<|endoftext|>\"}):\n",
    "        \"\"\"\n",
    "        Train the BPE tokenizer from scratch.\n",
    "\n",
    "        Args:\n",
    "            text (str): The training text.\n",
    "            vocab_size (int): The desired vocabulary size.\n",
    "            allowed_special (set): A set of special tokens to include.\n",
    "        \"\"\"\n",
    "\n",
    "        # Preprocess: Replace spaces with \"Ġ\"\n",
    "        # Note that Ġ is a particularity of the GPT-2 BPE implementation\n",
    "        # E.g., \"Hello world\" might be tokenized as [\"Hello\", \"Ġworld\"]\n",
    "        # (GPT-4 BPE would tokenize it as [\"Hello\", \" world\"])\n",
    "        \n",
    "        processed_text = text\n",
    "        '''\n",
    "        processed_text = []\n",
    "        for i, char in enumerate(text):\n",
    "            if char == \" \" and i != 0:\n",
    "                processed_text.append(\"Ġ\")\n",
    "            if char != \" \":\n",
    "                processed_text.append(char)\n",
    "        processed_text = \"\".join(processed_text)\n",
    "        '''\n",
    "\n",
    "        # Initialize vocab with unique characters, including \"Ġ\" if present\n",
    "        # Start with the first 256 ASCII characters\n",
    "        unique_chars = [chr(i) for i in range(256)]\n",
    "        unique_chars.extend(\n",
    "            char for char in sorted(set(processed_text))\n",
    "            if char not in unique_chars\n",
    "        )\n",
    "\n",
    "        '''\n",
    "        if \"Ġ\" not in unique_chars:\n",
    "            unique_chars.append(\"Ġ\")\n",
    "        '''\n",
    "\n",
    "        self.vocab = {i: char for i, char in enumerate(unique_chars)}\n",
    "        self.inverse_vocab = {char: i for i, char in self.vocab.items()}\n",
    "\n",
    "        # Add allowed special tokens\n",
    "        if allowed_special:\n",
    "            for token in allowed_special:\n",
    "                if token not in self.inverse_vocab:\n",
    "                    new_id = len(self.vocab)\n",
    "                    self.vocab[new_id] = token\n",
    "                    self.inverse_vocab[token] = new_id\n",
    "\n",
    "        # Tokenize the processed_text into token IDs\n",
    "        token_ids = [self.inverse_vocab[char] for char in processed_text]\n",
    "\n",
    "        # BPE steps 1-3: Repeatedly find and replace frequent pairs\n",
    "        for new_id in range(len(self.vocab), vocab_size):\n",
    "            pair_id = self.find_freq_pair(token_ids, mode=\"most\")\n",
    "            if pair_id is None:\n",
    "                break\n",
    "            token_ids = self.replace_pair(token_ids, pair_id, new_id)\n",
    "            self.bpe_merges[pair_id] = new_id\n",
    "\n",
    "        # Build the vocabulary with merged tokens\n",
    "        for (p0, p1), new_id in self.bpe_merges.items():\n",
    "            merged_token = self.vocab[p0] + self.vocab[p1]\n",
    "            self.vocab[new_id] = merged_token\n",
    "            self.inverse_vocab[merged_token] = new_id\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    def load_vocab_and_merges_from_openai(self, vocab_path, bpe_merges_path):\n",
    "        \"\"\"\n",
    "        Load pre-trained vocabulary and BPE merges from OpenAI's GPT-2 files.\n",
    "\n",
    "        Args:\n",
    "            vocab_path (str): Path to the vocab file (GPT-2 calls it 'encoder.json').\n",
    "            bpe_merges_path (str): Path to the bpe_merges file  (GPT-2 calls it 'vocab.bpe').\n",
    "        \"\"\"\n",
    "        # Load vocabulary\n",
    "        with open(vocab_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            loaded_vocab = json.load(file)\n",
    "            # Convert loaded vocabulary to correct format\n",
    "            self.vocab = {int(v): k for k, v in loaded_vocab.items()}\n",
    "            self.inverse_vocab = {k: int(v) for k, v in loaded_vocab.items()}\n",
    "\n",
    "        # Handle newline character without adding a new token\n",
    "        if \"\\n\" not in self.inverse_vocab:\n",
    "            # Use an existing token ID as a placeholder for '\\n'\n",
    "            # Preferentially use \"<|endoftext|>\" if available\n",
    "            fallback_token = next((token for token in [\"<|endoftext|>\", \"Ġ\", \"\"] if token in self.inverse_vocab), None)\n",
    "            if fallback_token is not None:\n",
    "                newline_token_id = self.inverse_vocab[fallback_token]\n",
    "            else:\n",
    "                # If no fallback token is available, raise an error\n",
    "                raise KeyError(\"No suitable token found in vocabulary to map '\\\\n'.\")\n",
    "\n",
    "            self.inverse_vocab[\"\\n\"] = newline_token_id\n",
    "            self.vocab[newline_token_id] = \"\\n\"\n",
    "\n",
    "        # Load GPT-2 merges and store them with an assigned \"rank\"\n",
    "        self.bpe_ranks = {}  # reset ranks\n",
    "        with open(bpe_merges_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            lines = file.readlines()\n",
    "            if lines and lines[0].startswith(\"#\"):\n",
    "                lines = lines[1:]\n",
    "\n",
    "            rank = 0\n",
    "            for line in lines:\n",
    "                pair = tuple(line.strip().split())\n",
    "                if len(pair) == 2:\n",
    "                    token1, token2 = pair\n",
    "                    # If token1 or token2 not in vocab, skip\n",
    "                    if token1 in self.inverse_vocab and token2 in self.inverse_vocab:\n",
    "                        self.bpe_ranks[(token1, token2)] = rank\n",
    "                        rank += 1\n",
    "                    else:\n",
    "                        print(f\"Skipping pair {pair} as one token is not in the vocabulary.\")\n",
    "    '''\n",
    "\n",
    "    def encode(self, text, allowed_special=None):\n",
    "        \"\"\"\n",
    "        Encode the input text into a list of token IDs, with tiktoken-style handling of special tokens.\n",
    "    \n",
    "        Args:\n",
    "            text (str): The input text to encode.\n",
    "            allowed_special (set or None): Special tokens to allow passthrough. If None, special handling is disabled.\n",
    "    \n",
    "        Returns:\n",
    "            List of token IDs.\n",
    "        \"\"\"\n",
    "        import re\n",
    "    \n",
    "        token_ids = []\n",
    "    \n",
    "        # If special token handling is enabled\n",
    "        if allowed_special is not None and len(allowed_special) > 0:\n",
    "            # Build regex to match allowed special tokens\n",
    "            special_pattern = (\n",
    "                \"(\" + \"|\".join(re.escape(tok) for tok in sorted(allowed_special, key=len, reverse=True)) + \")\"\n",
    "            )\n",
    "    \n",
    "            last_index = 0\n",
    "            for match in re.finditer(special_pattern, text):\n",
    "                prefix = text[last_index:match.start()]\n",
    "                token_ids.extend(self.encode(prefix, allowed_special=None))  # Encode prefix without special handling\n",
    "    \n",
    "                special_token = match.group(0)\n",
    "                if special_token in self.inverse_vocab:\n",
    "                    token_ids.append(self.inverse_vocab[special_token])\n",
    "                else:\n",
    "                    raise ValueError(f\"Special token {special_token} not found in vocabulary.\")\n",
    "                last_index = match.end()\n",
    "    \n",
    "            text = text[last_index:]  # Remaining part to process normally\n",
    "    \n",
    "            # Check if any disallowed special tokens are in the remainder\n",
    "            disallowed = [\n",
    "                tok for tok in self.inverse_vocab\n",
    "                if tok.startswith(\"<|\") and tok.endswith(\"|>\") and tok in text and tok not in allowed_special\n",
    "            ]\n",
    "            if disallowed:\n",
    "                raise ValueError(f\"Disallowed special tokens encountered in text: {disallowed}\")\n",
    "    \n",
    "        # If no special tokens, or remaining text after special token split:\n",
    "        tokens = []\n",
    "        lines = text.split(\"\\n\")\n",
    "        for i, line in enumerate(lines):\n",
    "            if i > 0:\n",
    "                tokens.append(\"\\n\")\n",
    "            words = line.split()\n",
    "            for j, word in enumerate(words):\n",
    "                if j == 0 and i > 0:\n",
    "                    tokens.append(\" \" + word) ############################################################\n",
    "                elif j == 0:\n",
    "                    tokens.append(word)\n",
    "                else:\n",
    "                    tokens.append(\" \" + word) ############################################################\n",
    "    \n",
    "        for token in tokens:\n",
    "            if token in self.inverse_vocab:\n",
    "                token_ids.append(self.inverse_vocab[token])\n",
    "            else:\n",
    "                token_ids.extend(self.tokenize_with_bpe(token))\n",
    "    \n",
    "        return token_ids\n",
    "\n",
    "    def tokenize_with_bpe(self, token):\n",
    "        \"\"\"\n",
    "        Tokenize a single token using BPE merges.\n",
    "\n",
    "        Args:\n",
    "            token (str): The token to tokenize.\n",
    "\n",
    "        Returns:\n",
    "            List[int]: The list of token IDs after applying BPE.\n",
    "        \"\"\"\n",
    "        # Tokenize the token into individual characters (as initial token IDs)\n",
    "        token_ids = [self.inverse_vocab.get(char, None) for char in token]\n",
    "        if None in token_ids:\n",
    "            missing_chars = [char for char, tid in zip(token, token_ids) if tid is None]\n",
    "            raise ValueError(f\"Characters not found in vocab: {missing_chars}\")\n",
    "\n",
    "        can_merge = True\n",
    "        while can_merge and len(token_ids) > 1:\n",
    "            can_merge = False\n",
    "            new_tokens = []\n",
    "            i = 0\n",
    "            while i < len(token_ids) - 1:\n",
    "                pair = (token_ids[i], token_ids[i + 1])\n",
    "                if pair in self.bpe_merges:\n",
    "                    merged_token_id = self.bpe_merges[pair]\n",
    "                    new_tokens.append(merged_token_id)\n",
    "                    i += 2  # Skip the next token as it's merged\n",
    "                    can_merge = True\n",
    "                else:\n",
    "                    new_tokens.append(token_ids[i])\n",
    "                    i += 1\n",
    "            if i < len(token_ids):\n",
    "                new_tokens.append(token_ids[i])\n",
    "            token_ids = new_tokens\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"\n",
    "        Decode a list of token IDs back into a string.\n",
    "\n",
    "        Args:\n",
    "            token_ids (List[int]): The list of token IDs to decode.\n",
    "\n",
    "        Returns:\n",
    "            str: The decoded string.\n",
    "        \"\"\"\n",
    "        decoded_string = \"\"\n",
    "        for i, token_id in enumerate(token_ids):\n",
    "            if token_id not in self.vocab:\n",
    "                raise ValueError(f\"Token ID {token_id} not found in vocab.\")\n",
    "            token = self.vocab[token_id]\n",
    "            if token == \"\\n\":\n",
    "                if decoded_string and not decoded_string.endswith(\" \"):\n",
    "                    decoded_string += \" \"  # Add space if not present before a newline\n",
    "                decoded_string += token\n",
    "\n",
    "            #elif token.startswith(\"Ġ\"):\n",
    "            #    decoded_string += \" \" + token[1:]\n",
    "\n",
    "            else:\n",
    "                decoded_string += token\n",
    "        return decoded_string\n",
    "\n",
    "    @staticmethod\n",
    "    def find_freq_pair(token_ids, mode=\"most\"):\n",
    "        pairs = Counter(zip(token_ids, token_ids[1:]))\n",
    "\n",
    "        if not pairs:\n",
    "            return None\n",
    "\n",
    "        if mode == \"most\":\n",
    "            return max(pairs.items(), key=lambda x: x[1])[0]\n",
    "        elif mode == \"least\":\n",
    "            return min(pairs.items(), key=lambda x: x[1])[0]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode. Choose 'most' or 'least'.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def replace_pair(token_ids, pair_id, new_id):\n",
    "        dq = deque(token_ids)\n",
    "        replaced = []\n",
    "\n",
    "        while dq:\n",
    "            current = dq.popleft()\n",
    "            if dq and (current, dq[0]) == pair_id:\n",
    "                replaced.append(new_id)\n",
    "                # Remove the 2nd token of the pair, 1st was already removed\n",
    "                dq.popleft()\n",
    "            else:\n",
    "                replaced.append(current)\n",
    "\n",
    "        return replaced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b9faff",
   "metadata": {},
   "source": [
    "## BPE implementation walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3abccc1",
   "metadata": {},
   "source": [
    "### Training, encoding, and decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a1e300",
   "metadata": {},
   "source": [
    "First, let's consider some sample text as our training dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280e4fb5",
   "metadata": {},
   "source": [
    "- Next, let's initialize and train the BPE tokenizer with a vocabulary size of 1,000\n",
    "- Note that the vocabulary size is already 256 by default due to the byte values discussed earlier, so we are only \"learning\" 744 vocabulary entries (if we consider the `<|endoftext|>` special token and the `Ġ` whitespace token; so, that's 742 to be precise)\n",
    "- For comparison, the GPT-2 vocabulary is 50,257 tokens, the GPT-4 vocabulary is 100,256 tokens (`cl100k_base` in tiktoken), and GPT-4o uses 199,997 tokens (`o200k_base` in tiktoken); they have all much bigger training sets compared to our simple example text above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80adef09",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BPETokenizerSimple()\n",
    "tokenizer.train(moby_dick, vocab_size=1000, allowed_special={\"<|endoftext|>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b0bd24fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "[(995, ',\" '), (996, 'bra'), (997, 'ct'), (998, 'ive '), (999, 'see ')]\n"
     ]
    }
   ],
   "source": [
    "# investigate length of vocab (should be as long as our parameter for vocab_size, i.e., 1000)\n",
    "print(len(tokenizer.vocab))\n",
    "\n",
    "# investigate last five items in vocab\n",
    "print(list(tokenizer.vocab.items())[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9927eca",
   "metadata": {},
   "source": [
    "This vocabulary is created by merging 743 times (`= 1000 - len(range(0, 256)) - len(special_tokens) = 1000 - 256 - 1 = 743`). This means that the first 256 entries are single-character tokens. Next, let's use the created merges via the `encode` method to encode some text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cb6bea72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65, 32, 411, 677, 110, 32, 295, 111, 32, 900, 404, 32, 479, 464, 32, 97, 32, 836, 289, 578, 101, 32, 900, 404, 258, 345, 336, 32, 266, 121, 277, 272, 32, 900, 119, 46]\n"
     ]
    }
   ],
   "source": [
    "input_text = \"A person who never made a mistake never tried anything new.\"\n",
    "token_ids = tokenizer.encode(input_text)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bed1f60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65, 32, 411, 677, 110, 32, 295, 111, 32, 900, 404, 32, 479, 464, 32, 97, 32, 836, 289, 578, 101, 32, 900, 404, 258, 345, 336, 32, 266, 121, 277, 272, 32, 900, 119, 46, 60, 124, 270, 641, 102, 116, 441, 116, 124, 62]\n"
     ]
    }
   ],
   "source": [
    "input_text = \"A person who never made a mistake never tried anything new.<|endoftext|> \"\n",
    "token_ids = tokenizer.encode(input_text)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6087a45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65, 32, 411, 677, 110, 32, 295, 111, 32, 900, 404, 32, 479, 464, 32, 97, 32, 836, 289, 578, 101, 32, 900, 404, 258, 345, 336, 32, 266, 121, 277, 272, 32, 900, 119, 46, 256]\n"
     ]
    }
   ],
   "source": [
    "input_text = \"A person who never made a mistake never tried anything new.<|endoftext|> \"\n",
    "token_ids = tokenizer.encode(input_text, allowed_special={\"<|endoftext|>\"})\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f80ca8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 73\n",
      "Number of token IDs: 37\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of characters:\", len(input_text))\n",
    "print(\"Number of token IDs:\", len(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f336f7c",
   "metadata": {},
   "source": [
    "From the lengths above, we can see that a 73-character sentence was encoded into 37 token IDs, effectively cutting the input length roughly in half compared to a character-byte-based encoding. Note that the vocabulary itself is used in the `decode()` method, which allows us to map the token IDs back into text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "03d94c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A person who never made a mistake never tried anything new.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07672075",
   "metadata": {},
   "source": [
    "Iterating over each token ID can give us a better understanding of how the token IDs are decoded via the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3258b24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 -> A\n",
      "32 ->  \n",
      "411 -> per\n",
      "677 -> so\n",
      "110 -> n\n",
      "32 ->  \n",
      "295 -> wh\n",
      "111 -> o\n",
      "32 ->  \n",
      "900 -> ne\n",
      "404 -> ver\n",
      "32 ->  \n",
      "479 -> ma\n",
      "464 -> de\n",
      "32 ->  \n",
      "97 -> a\n",
      "32 ->  \n",
      "836 -> mi\n",
      "289 -> st\n",
      "578 -> ak\n",
      "101 -> e\n",
      "32 ->  \n",
      "900 -> ne\n",
      "404 -> ver\n",
      "258 ->  t\n",
      "345 -> ri\n",
      "336 -> ed\n",
      "32 ->  \n",
      "266 -> an\n",
      "121 -> y\n",
      "277 -> th\n",
      "272 -> ing\n",
      "32 ->  \n",
      "900 -> ne\n",
      "119 -> w\n",
      "46 -> .\n",
      "256 -> <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "for token_id in token_ids:\n",
    "    print(f\"{token_id} -> {tokenizer.decode([token_id])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c800c73a",
   "metadata": {},
   "source": [
    "As we can see, most token IDs represent 2-character subwords; that's because the training data text is very short with not that many repetitive words, and because we used a relatively small vocabulary size. As a summary, calling `decode(encode())` should be able to reproduce arbitrary input texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "723f778a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is some text.'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(\n",
    "    tokenizer.encode(\"This is some text.\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6a831184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is some text with \\n newline characters.'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(\n",
    "    tokenizer.encode(\"This is some text with \\n newline characters.\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773f57a2",
   "metadata": {},
   "source": [
    "## OpenAI's open-source tiktoken library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa62354f",
   "metadata": {},
   "source": [
    "In practice, I highly recommend using [tiktoken](https://github.com/openai/tiktoken) because above implementation focuses on readability and educational purposes but not on performance. The BPE tokenizer from OpenAI's open-source [tiktoken](https://github.com/openai/tiktoken) library implements its core algorithms in Rust to improve computational performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48967a77-7d17-42bf-9e92-fc619d63a59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6ad3312f-a5f7-4efc-9d7d-8ea09d7b5128",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5ff2cd85-7cfb-4325-b390-219938589428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 30589, 39645, 14711, 7656, 357, 33, 11401, 8, 11241, 7509, 318, 257, 850, 4775, 11241, 1634, 11862, 326, 11629, 9404, 4017, 3212, 262, 749, 10792, 14729, 286, 3435, 393, 2095, 16311, 287, 257, 2420, 284, 1382, 257, 25818, 286, 2219, 850, 4775, 4991, 11, 15882, 6942, 290, 12846, 10552, 286, 2456, 13]\n"
     ]
    }
   ],
   "source": [
    "text = \"A Byte Pair Encoding (BPE) tokenizer is a subword tokenization algorithm that iteratively \" \\\n",
    "\"merges the most frequent pairs of characters or character sequences in a text to build a vocabulary of \" \\\n",
    "\"common subword units, enabling efficient and flexible representation of words.\"\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d26a48bb-f82e-41a8-a955-a1c9cf9d50ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Byte Pair Encoding (BPE) tokenizer is a subword tokenization algorithm that iteratively merges the most frequent pairs of characters or character sequences in a text to build a vocabulary of common subword units, enabling efficient and flexible representation of words.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c66368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15547734",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abbd7c0d-70f8-4386-a114-907e96c950b0",
   "metadata": {},
   "source": [
    "## 2.6 Data sampling with a sliding window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509d9826-6384-462e-aa8a-a7c73cd6aad0",
   "metadata": {},
   "source": [
    "- We train LLMs to generate one word at a time, so we want to prepare the training data accordingly where the next word in a sequence represents the target to predict:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fb44f4-0c43-4a6a-9c2f-9cf31452354c",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/12.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "848d5ade-fd1f-46c3-9e31-1426e315c71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebd0657-5543-43ca-8011-2ae6bd0a5810",
   "metadata": {},
   "source": [
    "- For each text chunk, we want the inputs and targets\n",
    "- Since we want the model to predict the next word, the targets are the inputs shifted by one position to the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e84424a7-646d-45b6-99e3-80d15fb761f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dfbff852-a92f-48c8-a46d-143a0f109f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815014ef-62f7-4476-a6ad-66e20e42b7c3",
   "metadata": {},
   "source": [
    "- One by one, the prediction would look like as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d97b031e-ed55-409d-95f2-aeb38c6fe366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f57bd746-dcbf-4433-8e24-ee213a8c34a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210d2dd9-fc20-4927-8d3d-1466cf41aae1",
   "metadata": {},
   "source": [
    "- We will take care of the next-word prediction in a later chapter after we covered the attention mechanism\n",
    "- For now, we implement a simple data loader that iterates over the input dataset and returns the inputs and targets shifted by one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a1b47a-f646-49d1-bc70-fddf2c840796",
   "metadata": {},
   "source": [
    "- Install and import PyTorch (see Appendix A for installation tips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e1770134-e7f3-4725-a679-e04c3be48cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9a3d50-885b-49bc-b791-9f5cc8bc7b7c",
   "metadata": {},
   "source": [
    "- We use a sliding window approach, changing the position by +1:\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/13.webp?123\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ac652d-7b38-4843-9fbd-494cdc8ec12c",
   "metadata": {},
   "source": [
    "- Create dataset and dataloader that extract chunks from the input text dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "74b41073-4c9f-46e2-a1bd-d38e4122b375",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "        assert len(token_ids) > max_length, \"Number of tokenized inputs must at least be equal to max_length+1\"\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5eb30ebe-97b3-43c5-9ff1-a97d621b3c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dd68ef-59f7-45ff-ba44-e311c899ddcd",
   "metadata": {},
   "source": [
    "- Let's test the dataloader with a batch size of 1 for an LLM with a context size of 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "df31d96c-6bfd-4564-a956-6192242d7579",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9226d00c-ad9a-4949-a6e4-9afccfc7214f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "10deb4bc-4de1-4d20-921e-4b1c7a0e1a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b006212f-de45-468d-bdee-5806216d1679",
   "metadata": {},
   "source": [
    "- An example using stride equal to the context length (here: 4) as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb467e0-bdcd-4dda-b9b0-a738c5d33ac3",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/14.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ae6d45-f26e-4b83-9c7b-cff55ffa7d16",
   "metadata": {},
   "source": [
    "- We can also create batched outputs\n",
    "- Note that we increase the stride here so that we don't have overlaps between the batches, since more overlap could lead to increased overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1916e7a6-f03d-4f09-91a6-d0bdbac5a58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd2fcda-2fda-4aa8-8bc8-de1e496f9db1",
   "metadata": {},
   "source": [
    "## 2.7 Creating token embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a301068-6ab2-44ff-a915-1ba11688274f",
   "metadata": {},
   "source": [
    "- The data is already almost ready for an LLM\n",
    "- But lastly let us embed the tokens in a continuous vector representation using an embedding layer\n",
    "- Usually, these embedding layers are part of the LLM itself and are updated (trained) during model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85089aa-8671-4e5f-a2b3-ef252004ee4c",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/15.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e014ca-1fc5-4b90-b6fa-c2097bb92c0b",
   "metadata": {},
   "source": [
    "- Suppose we have the following four input examples with input ids 2, 3, 5, and 1 (after tokenization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "15a6304c-9474-4470-b85d-3991a49fa653",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14da6344-2c71-4837-858d-dd120005ba05",
   "metadata": {},
   "source": [
    "- For the sake of simplicity, suppose we have a small vocabulary of only 6 words and we want to create embeddings of size 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "93cb2cee-9aa6-4bb8-8977-c65661d16eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff241f6-78eb-4e4a-a55f-5b2b6196d5b0",
   "metadata": {},
   "source": [
    "- This would result in a 6x3 weight matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a686eb61-e737-4351-8f1c-222913d47468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fcf4f5-0801-4eb4-bb90-acce87935ac7",
   "metadata": {},
   "source": [
    "- For those who are familiar with one-hot encoding, the embedding layer approach above is essentially just a more efficient way of implementing one-hot encoding followed by matrix multiplication in a fully-connected layer, which is described in the supplementary code in [./embedding_vs_matmul](../03_bonus_embedding-vs-matmul)\n",
    "- Because the embedding layer is just a more efficient implementation that is equivalent to the one-hot encoding and matrix-multiplication approach it can be seen as a neural network layer that can be optimized via backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0d58c3-83c0-4205-aca2-9c48b19fd4a7",
   "metadata": {},
   "source": [
    "- To convert a token with id 3 into a 3-dimensional vector, we do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e43600ba-f287-4746-8ddf-d0f71a9023ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bbf625-4f36-491d-87b4-3969efb784b0",
   "metadata": {},
   "source": [
    "- Note that the above is the 4th row in the `embedding_layer` weight matrix\n",
    "- To embed all four `input_ids` values above, we do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "50280ead-0363-44c8-8c35-bb885d92c8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be97ced4-bd13-42b7-866a-4d699a17e155",
   "metadata": {},
   "source": [
    "- An embedding layer is essentially a look-up operation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33c2741-bf1b-4c60-b7fd-61409d556646",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/16.webp?123\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08218d9f-aa1a-4afb-a105-72ff96a54e73",
   "metadata": {},
   "source": [
    "- **You may be interested in the bonus content comparing embedding layers with regular linear layers: [../03_bonus_embedding-vs-matmul](../03_bonus_embedding-vs-matmul)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c393d270-b950-4bc8-99ea-97d74f2ea0f6",
   "metadata": {},
   "source": [
    "## 2.8 Encoding word positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24940068-1099-4698-bdc0-e798515e2902",
   "metadata": {},
   "source": [
    "- Embedding layer convert IDs into identical vector representations regardless of where they are located in the input sequence:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0b14a2-f3f3-490e-b513-f262dbcf94fa",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/17.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a7d7fe-38a5-46e6-8db6-b688887b0430",
   "metadata": {},
   "source": [
    "- Positional embeddings are combined with the token embedding vector to form the input embeddings for a large language model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48de37db-d54d-45c4-ab3e-88c0783ad2e4",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/18.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f187f87-c1f8-4c2e-8050-350bbb972f55",
   "metadata": {},
   "source": [
    "- The BytePair encoder has a vocabulary size of 50,257:\n",
    "- Suppose we want to encode the input tokens into a 256-dimensional vector representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0b9e344d-03a6-4f2c-b723-67b6a20c5041",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2654722-24e4-4b0d-a43c-436a461eb70b",
   "metadata": {},
   "source": [
    "- If we sample data from the dataloader, we embed the tokens in each batch into a 256-dimensional vector\n",
    "- If we have a batch size of 8 with 4 tokens each, this results in a 8 x 4 x 256 tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ad56a263-3d2e-4d91-98bf-d0b68d3c7fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length,\n",
    "    stride=max_length, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "84416b60-3707-4370-bcbc-da0b62f2b64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7766ec38-30d0-4128-8c31-f49f063c43d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)\n",
    "\n",
    "# uncomment & execute the following line to see how the embeddings look like\n",
    "# print(token_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2ae164-6f19-4e32-b9e5-76950fcf1c9f",
   "metadata": {},
   "source": [
    "- GPT-2 uses absolute position embeddings, so we just create another embedding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cc048e20-7ac8-417e-81f5-8fe6f9a4fe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "\n",
    "# uncomment & execute the following line to see how the embedding layer weights look like\n",
    "# print(pos_embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c369a1e7-d566-4b53-b398-d6adafb44105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)\n",
    "\n",
    "# uncomment & execute the following line to see how the embeddings look like\n",
    "# print(pos_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870e9d9f-2935-461a-9518-6d1386b976d6",
   "metadata": {},
   "source": [
    "- To create the input embeddings used in an LLM, we simply add the token and the positional embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b22fab89-526e-43c8-9035-5b7018e34288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)\n",
    "\n",
    "# uncomment & execute the following line to see how the embeddings look like\n",
    "# print(input_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbda581-6f9b-476f-8ea7-d244e6a4eaec",
   "metadata": {},
   "source": [
    "- In the initial phase of the input processing workflow, the input text is segmented into separate tokens\n",
    "- Following this segmentation, these tokens are transformed into token IDs based on a predefined vocabulary:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bb0f7e-460d-44db-b366-096adcd84fff",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/19.webp\" width=\"400px\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
