{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83e5ef82",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ffffff; color: #000000; padding: 10px;\">\n",
    "<div style=\"display: flex; justify-content: space-between; align-items: center; background-color: #ffffff; color: #000000; padding: 10px;\">\n",
    "    <img src=\"../media/logo_kisz.png\" height=\"80\" style=\"margin-right: auto;\" alt=\"Logo of the AI Service Center Berlin-Brandenburg.\">\n",
    "    <img src=\"../media/logo_bmbf.jpeg\" height=\"150\" style=\"margin-left: auto;\" alt=\"Logo of the German Federal Ministry of Education and Research: Gefördert vom Bundesministerium für Bildung und Forschung.\">\n",
    "</div>\n",
    "<h1> Video Search\n",
    "<h2> Finding Locations in Audio and Video with Automatic Speech Recognition and Semantic Search\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aa40e3-5109-433f-9153-f5770531fe94",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f6a800; color: #ffffff; padding: 10px;\">\n",
    "    <h2> Part 2 - Byte Pair Encoding\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1641f131",
   "metadata": {},
   "source": [
    "When working with OpenAI's Whisper or Large Language Models (LLMs) in general, we need to represent text in a format that neural networks can understand. Word embeddings provide this format by converting discrete language units (e.g., words, punctuation) into continuous vectors of real numbers (e.g., `[-0.342, 0.234, 0.633, 0.451]`). These vectors, so called *embeddings*, capture semantic and syntactic information, enabling models to recognise patterns, similarities, and relationships in language data.\n",
    "\n",
    "The making of embeddings can be divided into two steps:\n",
    "\n",
    "1. Tokenisation of training data (e.g., text), so that each unique token (e.g., word or punctuation character) can be represented with a number.\n",
    "2. Computing embeddings with one of various methods (e.g., next word prediction), which is often part of the model training.\n",
    "\n",
    "This notebook introduces the tokenisation of text and in particular the Byte Pair Encoder (BPE), which is used by many contemporary LLMs. The next notebook - [Part 3: Embeddings](03_embeddings.ipynb) - describes how to compute embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddbb984-8d23-40c5-bbfa-c3c379e7eec3",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #dd6108; color: #ffffff; padding: 10px;\">\n",
    "<h3>1. Tokenisation - Breaking down Text into small Units\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c90731-7dc9-4cd3-8c4a-488e33b48e80",
   "metadata": {},
   "source": [
    "In order to compute embeddings, we first need to tokenise the text that constitutes the training data. Tokenisation is the process of breaking down continuous texts into smaller units such as words and punctuation characters. Each unique token will get its own ID. In the following, tokenisation is illustrated in the context of the novel *Moby Dick*, which can be downloaded from the *Gutenberg Corpus*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ab7066b20c97429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a07bf5859d36d5",
   "metadata": {},
   "source": [
    "If the download does not work, use the solution from stackoverflow: https://stackoverflow.com/questions/38916452/nltk-download-ssl-certificate-verify-failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40f9d9b1-6d32-485a-825a-a95392a86d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"meet a whale - ship on the ocean without being struck by her near appearance . The vessel under shor\"\n",
      "Total number of characters in the novel: 80\n",
      "Total number of unique words in the novel: 19317\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# get text from novel Moby Dick\n",
    "moby_dick = gutenberg.words('melville-moby_dick.txt')\n",
    "moby_dick = ' '.join(moby_dick)\n",
    "\n",
    "# print a few words to understand the data's structure\n",
    "print(f'\"{moby_dick[20000:20100]}\"')\n",
    "print(\"Total number of characters in the novel:\", len(set(moby_dick)))\n",
    "print(\"Total number of unique words in the novel:\", len(set(moby_dick.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f619fcd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of characters per sentence: 164.71\n",
      "Average number of words per sentence: 33.79\n"
     ]
    }
   ],
   "source": [
    "# Split the text into sentences using \".\" as separator\n",
    "sentences = moby_dick.split(\".\")\n",
    "\n",
    "# Remove empty sentences (e.g., trailing after last \".\")\n",
    "sentences = [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "# Calculate average number of characters per sentence\n",
    "avg_chars = sum(len(s) for s in sentences) / len(sentences)\n",
    "\n",
    "# Calculate average number of words per sentence (split on space)\n",
    "avg_words = sum(len(s.split()) for s in sentences) / len(sentences)\n",
    "\n",
    "print(f\"Average number of characters per sentence: {avg_chars:.2f}\")\n",
    "print(f\"Average number of words per sentence: {avg_words:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97811f16",
   "metadata": {},
   "source": [
    "For creating tokens, we first split the text into words and punctuation characters. Each of these *units* will be the basis for a token. These units will not be our final tokens, because some units will be further decomposed, as is detailled out below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bb193eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Moby', 'Dick', 'by', 'Herman', 'Melville', '1851', ']', 'ETYMOLOGY', '.', '(', 'Supplied', 'by', 'a', 'Late', 'Consumptive', 'Usher', 'to', 'a', 'Grammar', 'School', ')', 'The', 'pale', 'Usher', '--', 'threadbare', 'in', 'coat', ',']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# split the text into tokens; each space seperated character string and each punctuation character is a unit\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', moby_dick)\n",
    "\n",
    "# remove empty tokens\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "# investigate\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5204973-f414-4c0d-87b0-cfec1f06e6ff",
   "metadata": {},
   "source": [
    "Next, we convert the text tokens into token IDs, representing our vocabulary. The vocabulary consists of each unique token, sorted alphabetically. Each unique token is then mapped onto its ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fdf0533-5ab6-42a5-83fa-a3b045de6396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 19243 unique tokens.\n",
      "('!', 0)\n",
      "('\"', 1)\n",
      "('$', 2)\n",
      "('&', 3)\n",
      "(\"'\", 4)\n",
      "('(', 5)\n",
      "(')', 6)\n",
      "('*', 7)\n",
      "(',', 8)\n",
      "('-', 9)\n",
      "('--', 10)\n"
     ]
    }
   ],
   "source": [
    "# create the vocabulary\n",
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(f'vocabulary size: {vocab_size} unique tokens.')\n",
    "\n",
    "# print the first items of the vocabulary\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b821ef8-4d53-43b6-a2b2-aef808c343c7",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #dd6108; color: #ffffff; padding: 10px;\">\n",
    "<h3>2. Special Context Tokens\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d709d57-2486-4152-b7f9-d3e4bd8634cd",
   "metadata": {},
   "source": [
    "Often, special tokens are added to the vocabulary for different reasons.\n",
    "- `<|UNK|>` (unknown word) denotes out-of-vocabulary words\n",
    "- `<|endoftext|>` or `<|EOS|>` (end of sequence) are used, among others, when multiple texts such as newspaper articles are concatenated\n",
    "- `<|beginningoftext|>` or `<|BOS|>` denote beginnings analogous to `<|endoftext|>` and `<|EOS|>`\n",
    "- `<|PAD|>` (padding) is used when training LLMs with batch sizes greater than 1 (i.e., inputs with different length are 'padded' to the length of the longest input; e.g., `['Never', 'mind', '<|PAD|>', '<|PAD|>', '<|PAD|>']` has the same length as `['Actions', 'speak', 'louder', 'than', 'words']`\n",
    "\n",
    "Next, the `<|UNK|>` token is added to our vocabulary. Without this special token, any out-of-vocabulary word would cause errors or be ignored during tokenisation and model inference. Because of this, almost each tokenisation approach uses the `<|UNK|>` token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce9df29c-6c5b-43f1-8c1a-c7f7b79db78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('zone', 19239)\n",
      "('zoned', 19240)\n",
      "('zones', 19241)\n",
      "('zoology', 19242)\n",
      "('<|UNK|>', 19243)\n"
     ]
    }
   ],
   "source": [
    "# add <|unk|> to vocabulary\n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|UNK|>\"])\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "\n",
    "# check whether it worked\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee7a1e5-b54f-4ca1-87ef-3d663c4ee1e7",
   "metadata": {},
   "source": [
    "We can now define `encode` and `decode` functions for a simple tokeniser. In the AI domain, encoding refers to the process of transforming an input sequence into tokens, whereas decoding refers to the reverse transformation, that is, tokens to output sequence. In the case of this notebook, input and output sequences are texts. With the `encode` and the `decode` function, we can turn text into IDs and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "948861c5-3f30-4712-a234-725f20d26f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    \n",
    "    # split text into tokens and add <|UNK|> for unknown tokens\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int \n",
    "            else \"<|UNK|>\" for item in preprocessed # add <|UNK|> for unknown tokens\n",
    "        ]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    # decode ids back to text\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab89baf",
   "metadata": {},
   "source": [
    "The tokeniser can be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d786a6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this sentence is a test sentence.\n",
      "[5, 3, 2, 1, 4, 3, 0]\n"
     ]
    }
   ],
   "source": [
    "# create a small vocabulary for testing\n",
    "sentence = sorted(set(['this', 'sentence', 'is', 'a', 'test', 'sentence', '.']))\n",
    "vocab_small = {token:integer for integer,token in enumerate(sentence)}\n",
    "\n",
    "# create tokenizer with the small vocabulary\n",
    "tokenizer = SimpleTokenizer(vocab_small)\n",
    "\n",
    "# tokenise the example sentence and decode it back to text\n",
    "ids = tokenizer.encode('this sentence is a test sentence.')\n",
    "print(tokenizer.decode(ids))\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f24fc78",
   "metadata": {},
   "source": [
    "After sorting alphabetically, the \".\" is the first token, which gets the ID 0. The second token (ID 1) is \"a\". The token \"sentence\", which occurs twice, gets the ID 3. Using the small vocabulary, the relationship between tokens and IDs is straightforward. However, in real texts that are much longer, it is not possible anymore to see this relationship at one glance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "647364ec-7995-4654-9b4a-7607ccf5f1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ship on the ocean without being struck by her near appearance. The vessel under short sail, with look - outs at the mast - heads, eagerly scanning the wide expanse around them, has\n",
      "[15641, 12867, 17275, 12802, 19050, 4821, 16694, 5471, 9992, 12548, 4227, 11, 3296, 18487, 17981, 15683, 15152, 8, 19037, 11657, 9, 12993, 4435, 17275, 11960, 9, 9897, 8, 7843, 15255, 17275, 18965, 8389, 4322, 17280, 8, 9839]\n"
     ]
    }
   ],
   "source": [
    "# build tokenizer with the vocabulary from Moby Dick\n",
    "tokenizer = SimpleTokenizer(vocab)\n",
    "\n",
    "# encode a part of the text and decode it back to text\n",
    "text = moby_dick[20015:20200]\n",
    "ids = tokenizer.encode(text)\n",
    "print(tokenizer.decode(ids))\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa728dd1-9d35-4ac7-938f-d411d73083f6",
   "metadata": {},
   "source": [
    "Out-of-vocabulary words are encoded with the ID that represents the special token `<|UNK|>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4133c502-18ac-4412-9f43-01caf4efa3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|UNK|> World\n",
      "[19243, 3640]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizer(vocab)\n",
    "\n",
    "text = \"Hello World\"\n",
    "\n",
    "print(tokenizer.decode(tokenizer.encode(text)))\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4ba34b-170f-4e71-939b-77aabb776f14",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #dd6108; color: #ffffff; padding: 10px;\">\n",
    "<h3>3. Byte Pair Encoding - Motivation\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21223687",
   "metadata": {},
   "source": [
    "When training LLMs, the way we represent text as tokens has a significant impact on both model performance and computational efficiency. Using a simple byte-level encoding, where each character is assigned its own token, results in very long input sequences, which increases memory usage and slows down training. In contrast, Byte Pair Encoding (BPE) builds a vocabulary of frequently occurring character sequences (subwords), allowing common words and patterns to be represented by fewer tokens. For instance, the words \"runs\" and \"running\" could be represented as [\"run\", \"s\"] and [\"run\", \"ing\"]. This has the following advantages:\n",
    "\n",
    "- **Optimised vocabulary size and computing costs:**  \n",
    "  A smaller vocabulary enables the model to represent more words with fewer tokens, reducing sequence length and speeding up training. Smaller vocabularies reduce the size of the embedding and output layers, leading to smaller memory requirements and faster computations. BPE is optimised for learning a manageable vocabulary of subwords, keeping both sequence lengths and model size reasonable.\n",
    "\n",
    "- **Smart usage of repetitive patterns in words:**  \n",
    "  Many words share common roots or affixes (e.g., \"run\", \"runs\", \"runner\", \"running\"). BPE exploits these repetitive patterns by merging frequent character pairs into subword units. This enables the model to efficiently represent related words using shared subword tokens, improving generalisation and reducing the number of unique tokens needed.\n",
    "\n",
    "- **Efficiently handling infrequent words:**  \n",
    "  Natural language exhibits a long-tail distribution: a small number of words are very common, while many words are rare or unique, which is especially the case of languages such as German or Dutch, where you can spontaneously build long compounds (e.g., \"Kaffeevollautomatreinigungsset\" or \"Aansprakelijkheidsverzekerig\"). BPE helps address this by breaking rare or unseen words into known subword units, allowing the model to handle out-of-vocabulary words gracefully without needing an excessively large vocabulary.\n",
    "\n",
    "The original BPE tokeniser that OpenAI implemented for training the original GPT models can be found [here](https://github.com/openai/gpt-2/blob/master/src/encoder.py). The BPE algorithm was originally described in 1994: \"[A New Algorithm for Data Compression](http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM)\" by Philip Gage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f22138",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #dd6108; color: #ffffff; padding: 10px;\">\n",
    "<h3>4. The Fundament of BPE - Bits and Bytes\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c4f887",
   "metadata": {},
   "source": [
    "The BPE tokeniser heavily relies on the notion of bits and bytes. So, what are bytes? A byte consists of eigth bits, so there are 2<sup>8</sup> = 256 possible values that a single byte can represent, ranging from 0 to 255. This is illustrated in the following code, which creates byte arrays for incrementaly longer sequences. As soon as the sequence gets longer than 256, an error is produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "649ff19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at n=257: byte must be in range(0, 256)\n"
     ]
    }
   ],
   "source": [
    "# Try to create bytearrays for incrementally longer sequences, print error when it occurs\n",
    "for n in range(0, 300):\n",
    "    try:\n",
    "        ba = bytearray(range(0, n))\n",
    "    except ValueError as e:\n",
    "        print(f\"Error at n={n}: {e}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e089d2d2",
   "metadata": {},
   "source": [
    "In principle, most texts can be converted into byte arrays, because texts hardly consist of more than 256 unique characters. This would be an easy approach, because converting a byte array into a list encodes each character with a unique IDs similar to the `encode` function that we defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bec42460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs:: [72, 101, 108, 108, 111, 32, 87, 111, 114, 108, 100]\n"
     ]
    }
   ],
   "source": [
    "# convert text into byte array\n",
    "text = \"Hello World\"\n",
    "byte_ary = bytearray(text, \"utf-8\")\n",
    "\n",
    "# enocde text\n",
    "ids = list(byte_ary)\n",
    "print(\"IDs::\", ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46abc48",
   "metadata": {},
   "source": [
    "The downside of this approach is that for each charater a unique ID is used. Thus, representing our 11-character sentence with byte-based tokens would result in a vector of length eleven, whereas representing this sentence with word-based tokens results in a vector of length two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "765ab6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 11\n",
      "Number of token IDs (byte-based): 11\n",
      "Number of token IDs (word-based): 2\n"
     ]
    }
   ],
   "source": [
    "# investigate length of encodings with byte-based and word-based tokens\n",
    "print(\"Number of characters:\", len(text))\n",
    "print(\"Number of token IDs (byte-based):\", len(ids))\n",
    "print(\"Number of token IDs (word-based):\", len(tokenizer.encode(text)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c774f1c0",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #dd6108; color: #ffffff; padding: 10px;\">\n",
    "<h3>5. BPE Algorithm\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cdc4f8",
   "metadata": {},
   "source": [
    "The BPE tokenisation algorithm builds a vocabulary consisting of character tokens, subword units, and words. Thus, commonly occurring subwords like `ent`, which can be found in, among others, \"entangle\", \"entertain\", \"enter\", \"entrance\", have their own token ID. The BPE algorithm strives to find the optimal balance between character and subword units. This is achieved as follows:\n",
    "\n",
    "**1. Identify frequent pairs**\n",
    "- In each iteration, scan the text to find the most commonly occurring pair of bytes (or characters)\n",
    "\n",
    "**2. Replace and record**\n",
    "\n",
    "- Replace that pair with a new placeholder ID (one not already in use, e.g., if we start with 0...255, the first placeholder would be 256)\n",
    "- Record this mapping in a lookup table\n",
    "- The size of the lookup table is a hyperparameter, also called \"vocabulary size\" (for GPT-2, that's\n",
    "50,257)\n",
    "\n",
    "**3. Repeat until no gains**\n",
    "\n",
    "- Keep repeating steps 1 and 2, continually merging the most frequent pairs\n",
    "- Stop when no further compression is possible (e.g., no pair occurs more than once)\n",
    "\n",
    "\n",
    "### Concrete example of the encoding part (steps 1 & 2)\n",
    "\n",
    "Suppose we have the text (training dataset) `the cat in the hat` from which we want to build the vocabulary for a BPE tokeniser.\n",
    "\n",
    "**Iteration 1**\n",
    "\n",
    "1. Identify frequent pairs\n",
    "  - In this text, \"th\" appears twice (at the beginning and before the second \"e\")\n",
    "\n",
    "2. Replace and record\n",
    "  - replace \"th\" with a new token ID that is not already in use, e.g., 256\n",
    "  - the new text is: `<256>e cat in <256>e hat`\n",
    "  - the new vocabulary is\n",
    "\n",
    "```\n",
    "  0: ...\n",
    "  ...\n",
    "  256: \"th\"\n",
    "```\n",
    "\n",
    "**Iteration 2**\n",
    "\n",
    "1. **Identify frequent pairs**  \n",
    "   - In the text `<256>e cat in <256>e hat`, the pair `<256>e` appears twice\n",
    "\n",
    "2. **Replace and record**  \n",
    "   - replace `<256>e` with a new token ID that is not already in use, for example, `257`.  \n",
    "   - The new text is:\n",
    "     ```\n",
    "     <257> cat in <257> hat\n",
    "     ```\n",
    "   - The updated vocabulary is:\n",
    "     ```\n",
    "     0: ...\n",
    "     ...\n",
    "     256: \"th\"\n",
    "     257: \"<256>e\"\n",
    "     ```\n",
    "\n",
    "**Iteration 3**\n",
    "\n",
    "1. **Identify frequent pairs**  \n",
    "   - In the text `<257> cat in <257> hat`, the pair `<257> ` appears twice (once at the beginning and once before “hat”).\n",
    "\n",
    "2. **Replace and record**  \n",
    "   - replace `<257> ` with a new token ID that is not already in use, for example, `258`.  \n",
    "   - the new text is:\n",
    "     ```\n",
    "     <258>cat in <258>hat\n",
    "     ```\n",
    "   - The updated vocabulary is:\n",
    "     ```\n",
    "     0: ...\n",
    "     ...\n",
    "     256: \"th\"\n",
    "     257: \"<256>e\"\n",
    "     258: \"<257> \"\n",
    "     ```\n",
    "     \n",
    "- and so forth\n",
    "\n",
    "### Concrete example of the decoding part\n",
    "\n",
    "- To restore the original text, we reverse the process by substituting each token ID with its corresponding pair in the reverse order they were introduced\n",
    "- Start with the final compressed text: `<258>cat in <258>hat`\n",
    "-  Substitute `<258>` → `<257> `: `<257> cat in <257> hat`  \n",
    "- Substitute `<257>` → `<256>e`: `<256>e cat in <256>e hat`\n",
    "- Substitute `<256>` → \"th\": `the cat in the hat`\n",
    "\n",
    "The result of this algorithm produces token IDs that represet characters, subwords, and words. For illustration, have a look at the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "685372b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298: ent\n",
      "318:  is\n",
      "617:  some\n",
      "1212: This\n",
      "2420:  text\n"
     ]
    }
   ],
   "source": [
    "# use the GPT-2 tokeniser, which is based on BPE\n",
    "import tiktoken\n",
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# 298 is the token \"ent\"\n",
    "print(f'{298}: {gpt2_tokenizer.decode([298])}')\n",
    "\n",
    "# here are a few words, that have their own token IDs\n",
    "for i in [318, 617, 1212, 2420]:\n",
    "    decoded = gpt2_tokenizer.decode([i])\n",
    "    print(f\"{i}: {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcbe67f",
   "metadata": {},
   "source": [
    "Below is an implementation of the BPE algorithm as a Python class that mimics the `tiktoken` Python user interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eac71d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, deque\n",
    "from functools import lru_cache\n",
    "\n",
    "\n",
    "class BPETokenizerSimple:\n",
    "    def __init__(self):\n",
    "        # Maps token_id to token_str (e.g., {11246: \"some\"})\n",
    "        self.vocab = {}\n",
    "        # Maps token_str to token_id (e.g., {\"some\": 11246})\n",
    "        self.inverse_vocab = {}\n",
    "        # Dictionary of BPE merges: {(token_id1, token_id2): merged_token_id}\n",
    "        self.bpe_merges = {}\n",
    "\n",
    "        # For the official OpenAI GPT-2 merges, use a rank dict:\n",
    "        # of form {(string_A, string_B): rank}, where lower rank = higher priority\n",
    "        self.bpe_ranks = {}\n",
    "\n",
    "    def train(self, text, vocab_size, allowed_special={\"<|endoftext|>\"}):\n",
    "        \"\"\"\n",
    "        Train the BPE tokenizer from scratch.\n",
    "\n",
    "        Args:\n",
    "            text (str): The training text.\n",
    "            vocab_size (int): The desired vocabulary size.\n",
    "            allowed_special (set): A set of special tokens to include.\n",
    "        \"\"\"\n",
    "        \n",
    "        processed_text = text\n",
    "\n",
    "        # Initialize vocab with unique characters, including \"Ġ\" if present\n",
    "        # Start with the first 256 ASCII characters\n",
    "        unique_chars = [chr(i) for i in range(256)]\n",
    "        unique_chars.extend(\n",
    "            char for char in sorted(set(processed_text))\n",
    "            if char not in unique_chars\n",
    "        )\n",
    "\n",
    "        self.vocab = {i: char for i, char in enumerate(unique_chars)}\n",
    "        self.inverse_vocab = {char: i for i, char in self.vocab.items()}\n",
    "\n",
    "        # Add allowed special tokens\n",
    "        if allowed_special:\n",
    "            for token in allowed_special:\n",
    "                if token not in self.inverse_vocab:\n",
    "                    new_id = len(self.vocab)\n",
    "                    self.vocab[new_id] = token\n",
    "                    self.inverse_vocab[token] = new_id\n",
    "\n",
    "        # Tokenize the processed_text into token IDs\n",
    "        token_ids = [self.inverse_vocab[char] for char in processed_text]\n",
    "\n",
    "        # BPE steps 1-3: Repeatedly find and replace frequent pairs\n",
    "        for new_id in range(len(self.vocab), vocab_size):\n",
    "            pair_id = self.find_freq_pair(token_ids, mode=\"most\")\n",
    "            if pair_id is None:\n",
    "                break\n",
    "            token_ids = self.replace_pair(token_ids, pair_id, new_id)\n",
    "            self.bpe_merges[pair_id] = new_id\n",
    "\n",
    "        # Build the vocabulary with merged tokens\n",
    "        for (p0, p1), new_id in self.bpe_merges.items():\n",
    "            merged_token = self.vocab[p0] + self.vocab[p1]\n",
    "            self.vocab[new_id] = merged_token\n",
    "            self.inverse_vocab[merged_token] = new_id\n",
    "\n",
    "    def encode(self, text, allowed_special=None):\n",
    "        \"\"\"\n",
    "        Encode the input text into a list of token IDs, with tiktoken-style handling of special tokens.\n",
    "    \n",
    "        Args:\n",
    "            text (str): The input text to encode.\n",
    "            allowed_special (set or None): Special tokens to allow passthrough. If None, special handling is disabled.\n",
    "    \n",
    "        Returns:\n",
    "            List of token IDs.\n",
    "        \"\"\"\n",
    "    \n",
    "        token_ids = []\n",
    "    \n",
    "        # If special token handling is enabled\n",
    "        if allowed_special is not None and len(allowed_special) > 0:\n",
    "            # Build regex to match allowed special tokens\n",
    "            special_pattern = (\n",
    "                \"(\" + \"|\".join(re.escape(tok) for tok in sorted(allowed_special, key=len, reverse=True)) + \")\"\n",
    "            )\n",
    "    \n",
    "            last_index = 0\n",
    "            for match in re.finditer(special_pattern, text):\n",
    "                prefix = text[last_index:match.start()]\n",
    "                token_ids.extend(self.encode(prefix, allowed_special=None))  # Encode prefix without special handling\n",
    "    \n",
    "                special_token = match.group(0)\n",
    "                if special_token in self.inverse_vocab:\n",
    "                    token_ids.append(self.inverse_vocab[special_token])\n",
    "                else:\n",
    "                    raise ValueError(f\"Special token {special_token} not found in vocabulary.\")\n",
    "                last_index = match.end()\n",
    "    \n",
    "            text = text[last_index:]  # Remaining part to process normally\n",
    "    \n",
    "            # Check if any disallowed special tokens are in the remainder\n",
    "            disallowed = [\n",
    "                tok for tok in self.inverse_vocab\n",
    "                if tok.startswith(\"<|\") and tok.endswith(\"|>\") and tok in text and tok not in allowed_special\n",
    "            ]\n",
    "            if disallowed:\n",
    "                raise ValueError(f\"Disallowed special tokens encountered in text: {disallowed}\")\n",
    "    \n",
    "        # If no special tokens, or remaining text after special token split:\n",
    "        tokens = []\n",
    "        lines = text.split(\"\\n\")\n",
    "        for i, line in enumerate(lines):\n",
    "            if i > 0:\n",
    "                tokens.append(\"\\n\")\n",
    "            words = line.split()\n",
    "            for j, word in enumerate(words):\n",
    "                if j == 0 and i <= 0:\n",
    "                    tokens.append(word)\n",
    "                else:\n",
    "                    tokens.append(\" \" + word)\n",
    "    \n",
    "        for token in tokens:\n",
    "            if token in self.inverse_vocab:\n",
    "                token_ids.append(self.inverse_vocab[token])\n",
    "            else:\n",
    "                token_ids.extend(self.tokenize_with_bpe(token))\n",
    "    \n",
    "        return token_ids\n",
    "\n",
    "    def tokenize_with_bpe(self, token):\n",
    "        \"\"\"\n",
    "        Tokenize a single token using BPE merges.\n",
    "\n",
    "        Args:\n",
    "            token (str): The token to tokenize.\n",
    "\n",
    "        Returns:\n",
    "            List[int]: The list of token IDs after applying BPE.\n",
    "        \"\"\"\n",
    "        # Tokenize the token into individual characters (as initial token IDs)\n",
    "        token_ids = [self.inverse_vocab.get(char, None) for char in token]\n",
    "        if None in token_ids:\n",
    "            missing_chars = [char for char, tid in zip(token, token_ids) if tid is None]\n",
    "            raise ValueError(f\"Characters not found in vocab: {missing_chars}\")\n",
    "\n",
    "        can_merge = True\n",
    "        while can_merge and len(token_ids) > 1:\n",
    "            can_merge = False\n",
    "            new_tokens = []\n",
    "            i = 0\n",
    "            while i < len(token_ids) - 1:\n",
    "                pair = (token_ids[i], token_ids[i + 1])\n",
    "                if pair in self.bpe_merges:\n",
    "                    merged_token_id = self.bpe_merges[pair]\n",
    "                    new_tokens.append(merged_token_id)\n",
    "                    i += 2  # Skip the next token as it's merged\n",
    "                    can_merge = True\n",
    "                else:\n",
    "                    new_tokens.append(token_ids[i])\n",
    "                    i += 1\n",
    "            if i < len(token_ids):\n",
    "                new_tokens.append(token_ids[i])\n",
    "            token_ids = new_tokens\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"\n",
    "        Decode a list of token IDs back into a string.\n",
    "\n",
    "        Args:\n",
    "            token_ids (List[int]): The list of token IDs to decode.\n",
    "\n",
    "        Returns:\n",
    "            str: The decoded string.\n",
    "        \"\"\"\n",
    "        decoded_string = \"\"\n",
    "        for i, token_id in enumerate(token_ids):\n",
    "            if token_id not in self.vocab:\n",
    "                raise ValueError(f\"Token ID {token_id} not found in vocab.\")\n",
    "            token = self.vocab[token_id]\n",
    "            if token == \"\\n\":\n",
    "                if decoded_string and not decoded_string.endswith(\" \"):\n",
    "                    decoded_string += \" \"  # Add space if not present before a newline\n",
    "                decoded_string += token\n",
    "            else:\n",
    "                decoded_string += token\n",
    "        return decoded_string\n",
    "\n",
    "    @staticmethod\n",
    "    def find_freq_pair(token_ids, mode=\"most\"):\n",
    "        pairs = Counter(zip(token_ids, token_ids[1:]))\n",
    "\n",
    "        if not pairs:\n",
    "            return None\n",
    "\n",
    "        if mode == \"most\":\n",
    "            return max(pairs.items(), key=lambda x: x[1])[0]\n",
    "        elif mode == \"least\":\n",
    "            return min(pairs.items(), key=lambda x: x[1])[0]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode. Choose 'most' or 'least'.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def replace_pair(token_ids, pair_id, new_id):\n",
    "        dq = deque(token_ids)\n",
    "        replaced = []\n",
    "\n",
    "        while dq:\n",
    "            current = dq.popleft()\n",
    "            if dq and (current, dq[0]) == pair_id:\n",
    "                replaced.append(new_id)\n",
    "                # Remove the 2nd token of the pair, 1st was already removed\n",
    "                dq.popleft()\n",
    "            else:\n",
    "                replaced.append(current)\n",
    "\n",
    "        return replaced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280e4fb5",
   "metadata": {},
   "source": [
    "Next, the BPE tokeniser is initated and trained with a vocabulary size of 1,000. Note that the vocabulary size is already 256 by default due to the byte values discussed earlier, so we are only \"learning\" 744 vocabulary entries For comparison, the GPT-2 vocabulary is 50,257 tokens, the GPT-4 vocabulary is 100,256 tokens (`cl100k_base` in tiktoken), and GPT-4o uses 199,997 tokens (`o200k_base` in tiktoken); they have all much bigger training sets compared to our simple example text above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80adef09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the tokeniser, which takes a minute or two, depending on your processing capacities\n",
    "tokenizer = BPETokenizerSimple()\n",
    "tokenizer.train(moby_dick, vocab_size=1000, allowed_special={\"<|endoftext|>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0bd24fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "[(995, ',\" '), (996, 'bra'), (997, 'ct'), (998, 'ive '), (999, 'see ')]\n"
     ]
    }
   ],
   "source": [
    "# investigate length of vocabulary (should be as long as our parameter for vocab_size, i.e., 1000)\n",
    "print(len(tokenizer.vocab))\n",
    "\n",
    "# investigate last five items in vocab\n",
    "print(list(tokenizer.vocab.items())[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9927eca",
   "metadata": {},
   "source": [
    "This vocabulary is created by merging 743 times (`= 1000 - len(range(0, 256)) - len(special_tokens) = 1000 - 256 - 1 = 743`). This means that the first 256 entries are single-character tokens. Next, let's use the created merges via the `encode` method to encode some text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb6bea72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65, 32, 411, 677, 110, 32, 295, 111, 32, 900, 404, 32, 479, 464, 32, 97, 32, 836, 289, 578, 101, 32, 900, 404, 258, 345, 336, 32, 266, 121, 277, 272, 32, 900, 119, 46]\n"
     ]
    }
   ],
   "source": [
    "input_text = \"A person who never made a mistake never tried anything new.\"\n",
    "token_ids = tokenizer.encode(input_text)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b63b9ce",
   "metadata": {},
   "source": [
    "We can enable the `<|endoftext|>` as follows. Note that the last token ID represents this special token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6087a45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65, 32, 411, 677, 110, 32, 295, 111, 32, 900, 404, 32, 479, 464, 32, 97, 32, 836, 289, 578, 101, 32, 900, 404, 258, 345, 336, 32, 266, 121, 277, 272, 32, 900, 119, 46, 256]\n"
     ]
    }
   ],
   "source": [
    "input_text = \"A person who never made a mistake never tried anything new.<|endoftext|> \"\n",
    "token_ids = tokenizer.encode(input_text, allowed_special={\"<|endoftext|>\"})\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f336f7c",
   "metadata": {},
   "source": [
    "The 73-character sentence was encoded into 37 token IDs, effectively cutting the input length roughly in half. How many byte pairs should be merged in order to yield the best performing vocabulary depends on the training data and use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f80ca8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 73\n",
      "Number of token IDs: 37\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of characters:\", len(input_text))\n",
    "print(\"Number of token IDs:\", len(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd80ac5e",
   "metadata": {},
   "source": [
    "The `decode()` method can be used to map token IDs back onto text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03d94c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A person who never made a mistake never tried anything new.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07672075",
   "metadata": {},
   "source": [
    "Iterating over each token ID can give us a better understanding of how the token IDs are decoded via the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3258b24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 -> A\n",
      "32 ->  \n",
      "411 -> per\n",
      "677 -> so\n",
      "110 -> n\n",
      "32 ->  \n",
      "295 -> wh\n",
      "111 -> o\n",
      "32 ->  \n",
      "900 -> ne\n",
      "404 -> ver\n",
      "32 ->  \n",
      "479 -> ma\n",
      "464 -> de\n",
      "32 ->  \n",
      "97 -> a\n",
      "32 ->  \n",
      "836 -> mi\n",
      "289 -> st\n",
      "578 -> ak\n",
      "101 -> e\n",
      "32 ->  \n",
      "900 -> ne\n",
      "404 -> ver\n",
      "258 ->  t\n",
      "345 -> ri\n",
      "336 -> ed\n",
      "32 ->  \n",
      "266 -> an\n",
      "121 -> y\n",
      "277 -> th\n",
      "272 -> ing\n",
      "32 ->  \n",
      "900 -> ne\n",
      "119 -> w\n",
      "46 -> .\n",
      "256 -> <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "for token_id in token_ids:\n",
    "    print(f\"{token_id} -> {tokenizer.decode([token_id])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c800c73a",
   "metadata": {},
   "source": [
    "As we can see, most token IDs represent 2-character subwords; that's because the training data text is very short with not that many repetitive words, and because we used a relatively small vocabulary size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773f57a2",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #dd6108; color: #ffffff; padding: 10px;\">\n",
    "<h3>6. OpenAI's Open-Source tiktoken Library\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa62354f",
   "metadata": {},
   "source": [
    "The BPE algorithm implementation above focuses on readability for educational purposes and is not recommended for training LLMs. Instead, I highly recommend using [tiktoken](https://github.com/openai/tiktoken), which implements its core algorithms in Rust to improve computational performance. The following cells illustrate how the tiktoken library can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48967a77-7d17-42bf-9e92-fc619d63a59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "# load the library\n",
    "import importlib\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ad3312f-a5f7-4efc-9d7d-8ea09d7b5128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the tokeniser\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ff2cd85-7cfb-4325-b390-219938589428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs: [32, 30589, 39645, 14711, 7656, 357, 33, 11401, 8, 11241, 5847, 318, 257, 850, 4775, 11241, 5612, 11862, 326, 11629, 9404, 4017, 3212, 262, 749, 10792, 14729, 286, 3435, 393, 2095, 16311, 287, 257, 2420, 284, 1382, 257, 25818, 286, 2219, 850, 4775, 4991, 11, 15882, 6942, 290, 12846, 10552, 286, 2456, 13]\n",
      "Decoded IDs: A Byte Pair Encoding (BPE) tokeniser is a subword tokenisation algorithm that iteratively merges the most frequent pairs of characters or character sequences in a text to build a vocabulary of common subword units, enabling efficient and flexible representation of words.\n"
     ]
    }
   ],
   "source": [
    "# use a sample text\n",
    "text = \"A Byte Pair Encoding (BPE) tokeniser is a subword tokenisation algorithm that iteratively \" \\\n",
    "\"merges the most frequent pairs of characters or character sequences in a text to build a vocabulary of \" \\\n",
    "\"common subword units, enabling efficient and flexible representation of words.\"\n",
    "\n",
    "# encode the text\n",
    "tiktoken_IDs = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(f'IDs: {tiktoken_IDs}')\n",
    "\n",
    "decoded_text = tokenizer.decode(tiktoken_IDs)\n",
    "print(f'Decoded IDs: {decoded_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580fe58d",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #dd6108; color: #ffffff; padding: 10px;\">\n",
    "<h3>7. Retrieving Embeddings from GloVe\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7f57e5",
   "metadata": {},
   "source": [
    "In this section, we retrieve **GloVe embeddings** for the tokens produced by our tokenizer.  \n",
    "\n",
    "**GloVe** (Global Vectors for Word Representation) is a widely used pre-trained word embedding model developed by Stanford University. It represents words as dense vectors in a high-dimensional space, capturing semantic relationships and similarities between words based on their co-occurrence statistics in large text corpora.\n",
    "\n",
    "Normally, in modern language models, embeddings are learned as part of the model training process. That is, the model learns to map each token to a vector representation that is optimal for the task. However, in this workshop, we do **not** train embeddings ourselves, as this is outside the scope of our current focus. Instead, we use pre-trained GloVe embeddings.\n",
    "\n",
    "If you are interested in a detailed, hands-on workshop on embeddings please see:  \n",
    "[https://github.com/aihpi/kisz-nlp-embeddings](https://github.com/aihpi/kisz-nlp-embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e313882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-07-07 10:57:56--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2025-07-07 10:57:57--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2025-07-07 10:57:58--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822,24M  2,01MB/s    in 13m 5s  \n",
      "\n",
      "2025-07-07 11:11:04 (1,05 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n",
      "Archive:  glove.6B.zip\n",
      "  inflating: glove.6B.50d.txt        \n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "  inflating: glove.6B.300d.txt       \n"
     ]
    }
   ],
   "source": [
    "# download and unzip GloVe embeddings\n",
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove.6B.zip\n",
    "!rm glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d1f1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create token strings from an example text\n",
    "text = moby_dick\n",
    "encoded_text = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "token_strings = [tokenizer.decode([tid]) for tid in encoded_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "527f4a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load GloVe embeddings\n",
    "glove_path = 'GloVe/glove.6B.300d.txt'\n",
    "glove_embeddings = {}\n",
    "with open(glove_path, 'r', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        word = parts[0]\n",
    "        vec = np.array(parts[1:], dtype=np.float32)\n",
    "        glove_embeddings[word] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e1ed81d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(290799, 300)\n"
     ]
    }
   ],
   "source": [
    "# Get embedding for each token string (lowercased, as GloVe is lowercased)\n",
    "embedding_dim = 300\n",
    "embeddings = []\n",
    "for token in token_strings:\n",
    "    # Remove whitespace and lowercase for GloVe lookup\n",
    "    key = token.strip().lower()\n",
    "    if key in glove_embeddings:\n",
    "        embeddings.append(glove_embeddings[key])\n",
    "    else:\n",
    "        # If not found, use a zero vector or random vector\n",
    "        embeddings.append(np.zeros(embedding_dim))\n",
    "\n",
    "embeddings = np.stack(embeddings)\n",
    "print(embeddings.shape)  # (num_tokens, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f77cf41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'ale': [-1.94409996e-01 -3.65069985e-01 -3.45169991e-01  4.92989987e-01\n",
      "  9.82309971e-03  3.84810001e-01 -5.14100015e-01  9.30480003e-01\n",
      "  3.82220000e-01  5.22710025e-01  3.29620004e-01 -1.14629996e+00\n",
      " -9.59580004e-01  1.48450002e-01  6.34609997e-01  3.42689991e-01\n",
      " -3.60170007e-01  2.94299990e-01 -4.70560014e-01 -3.04760009e-01\n",
      "  2.06379995e-01 -7.02880025e-02 -5.22329986e-01  6.26590014e-01\n",
      " -7.09309995e-01 -3.03290009e-01 -1.82789996e-01 -8.41019955e-03\n",
      " -5.58499992e-01  3.18789989e-01  4.81970012e-02 -2.08139997e-02\n",
      " -2.67840009e-02 -1.60840005e-01 -3.65869999e-01  6.82219982e-01\n",
      "  4.71410006e-01  7.21440017e-01  2.03740001e-01  8.18630010e-02\n",
      "  1.33589998e-01  2.05090001e-01  3.46089989e-01 -1.39920004e-02\n",
      "  5.48330009e-01 -5.43470025e-01 -1.40400007e-01 -6.86999977e-01\n",
      "  6.22420013e-02  2.56579995e-01 -3.30989987e-01 -5.27249992e-01\n",
      "  3.24649990e-01  2.70020008e-01  2.40099996e-01  3.60870004e-01\n",
      "  3.16729993e-01  2.57349998e-01 -6.90100014e-01  4.76429999e-01\n",
      "  2.49300003e-01 -6.19780004e-01  4.49490011e-01 -2.33109996e-01\n",
      " -3.67890000e-01 -2.90690005e-01 -6.80440009e-01 -2.23719999e-01\n",
      " -2.06070006e-01 -1.01240003e+00  4.72039998e-01  2.89700001e-01\n",
      " -5.69159985e-01 -6.46220028e-01  6.70149997e-02 -1.59850001e-01\n",
      "  4.83209997e-01 -2.47409999e-01 -8.45220029e-01  3.36369991e-01\n",
      "  1.57859996e-01  1.04560006e+00  8.28349963e-02 -7.69519985e-01\n",
      "  6.21510029e-01 -9.08609986e-01  1.16109997e-01  3.00619990e-01\n",
      "  1.05889998e-01 -9.94599983e-02 -3.31250012e-01 -8.39599967e-02\n",
      "  5.67560017e-01 -8.81289989e-02  1.01690002e-01  1.43539995e-01\n",
      "  4.77759987e-01 -7.16959983e-02  3.17439996e-02  1.04890001e+00\n",
      " -3.80840003e-01 -7.91570008e-01 -4.52940017e-02  4.79790002e-01\n",
      " -1.38310000e-01  6.80669993e-02 -1.60180002e-01  8.76580000e-01\n",
      " -3.59129995e-01  2.37639993e-01  3.09689999e-01 -3.59730013e-02\n",
      " -1.00110002e-01 -5.21629989e-01 -2.67329998e-02 -4.84290004e-01\n",
      " -5.07380009e-01  4.17179987e-02 -9.10819992e-02 -3.90309989e-01\n",
      " -5.08870006e-01  4.13889997e-02  4.99449998e-01 -2.21220002e-01\n",
      " -3.10339987e-01 -1.87999994e-01 -5.71380019e-01  6.15719974e-01\n",
      " -1.83709994e-01 -3.20670009e-01 -4.74249989e-01  6.03820026e-01\n",
      " -4.63070005e-01  1.23970002e-01 -2.12439999e-01  3.26739997e-01\n",
      " -6.09839976e-01  8.10779989e-01 -1.04050003e-02 -2.81830013e-01\n",
      "  1.10329998e+00  7.38799989e-01  3.07539999e-01 -3.89959991e-01\n",
      " -2.38419995e-02 -3.95680010e-01 -3.16520005e-01 -2.92130001e-02\n",
      "  2.97630012e-01  1.91300005e-01 -2.43100002e-01  3.26599985e-01\n",
      "  5.58930002e-02 -3.77260000e-01 -3.10200006e-01 -4.72810000e-01\n",
      "  5.80120027e-01 -3.63499999e-01  2.80620009e-01 -3.07700008e-01\n",
      "  7.27490008e-01  2.53670007e-01 -4.96419996e-01 -1.95759997e-01\n",
      " -3.40249985e-01  4.35360014e-01  2.81720012e-01 -5.66500016e-02\n",
      "  6.43130004e-01 -1.20549999e-01  1.15759999e-01  5.41890025e-01\n",
      "  2.38539994e-01  2.00670004e-01  5.91780007e-01  1.72429994e-01\n",
      " -1.40239999e-01 -9.23789978e-01  7.76979983e-01 -4.24970001e-01\n",
      " -5.01100004e-01 -2.30969995e-01 -1.74490005e-01  2.97509998e-01\n",
      "  6.35810018e-01 -1.73079997e-01  1.11940002e+00  1.34069994e-01\n",
      " -2.21910000e-01 -9.44949985e-02  2.54220009e-01  2.81919986e-01\n",
      " -2.03669995e-01 -1.23870000e-01 -2.83060014e-01 -2.86249995e-01\n",
      " -4.64480013e-01 -4.07420009e-01 -5.14479995e-01 -1.58849999e-01\n",
      " -2.29349993e-02  4.08289999e-01  3.52560014e-01  1.76780000e-01\n",
      " -1.98500007e-01 -7.84789994e-02 -4.16960001e-01  6.07969999e-01\n",
      " -1.01420000e-01 -1.64829995e-02  3.29109997e-01 -1.20099999e-01\n",
      "  4.03450012e-01  7.65259981e-01 -1.80680007e-01  2.56799996e-01\n",
      "  3.35180014e-01 -2.53809988e-01 -1.88240007e-01 -4.46369983e-02\n",
      "  5.81410006e-02 -2.53399998e-01 -1.13200000e-03 -2.17439998e-02\n",
      " -5.35769999e-01 -1.29700005e-01  7.68949986e-01 -4.23949987e-01\n",
      " -3.41509998e-01 -4.89320010e-01 -8.85200024e-01 -8.41480028e-03\n",
      " -1.23690002e-01  2.91249990e-01  3.49059999e-01  1.50079995e-01\n",
      "  3.21630001e-01 -2.81280011e-01 -4.64850008e-01 -3.80720012e-02\n",
      " -7.69729972e-01  3.39509994e-02 -7.94879973e-01 -6.08690023e-01\n",
      " -2.54339993e-01 -2.18940005e-02  1.21289998e-01  6.59659982e-01\n",
      " -2.12129995e-01 -7.71759987e-01  2.90589988e-01  2.10559994e-01\n",
      "  3.64840001e-01 -1.34029999e-01 -3.62799987e-02 -8.36369991e-02\n",
      "  2.66469985e-01 -2.68139988e-01  3.25480014e-01 -2.60679990e-01\n",
      "  5.09530008e-01  2.56749988e-01  1.85849994e-01  2.76870012e-01\n",
      " -1.87610000e-01 -3.55969995e-01  7.80040026e-01  2.11590007e-01\n",
      "  7.49700010e-01  5.02919972e-01  3.80339995e-02 -6.58360004e-01\n",
      "  3.55740011e-01 -4.79070008e-01 -2.00890005e-02  1.71619996e-01\n",
      " -2.87259996e-01 -4.26779985e-01 -9.24579978e-01  1.67539999e-01\n",
      " -2.01250002e-01  5.06510019e-01 -3.76500010e-01  3.79920006e-01\n",
      " -1.78619996e-01  3.28839988e-01 -3.43609989e-01 -1.57930002e-01\n",
      "  1.57250002e-01  1.30889997e-01  1.41130000e-01  1.08980000e+00\n",
      "  2.39460003e-02  4.81770009e-01  2.28310004e-01 -3.58429994e-03\n",
      " -1.35110006e-01  3.57840002e-01 -1.44189999e-01  6.85739994e-01]\n"
     ]
    }
   ],
   "source": [
    "# Find the index of a token\n",
    "my_token = \"fish\"\n",
    "my_token = \"whale\"\n",
    "my_token = \"wh\"\n",
    "my_token = \"ale\"\n",
    "\n",
    "try:\n",
    "    idx = token_strings.index(my_token)\n",
    "    idx_embedding = embeddings[idx]\n",
    "    print(f\"Embedding for '{my_token}':\", idx_embedding)\n",
    "except ValueError:\n",
    "    print(f\"'{my_token}' not found in tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309215cc",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #dd6108; color: #ffffff; padding: 10px;\">\n",
    "<h3>7. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31702d63",
   "metadata": {},
   "source": [
    "In this notebook, we explored the fundamentals of tokenisation for LLMs, focusing on the motivation and mechanics behind Byte Pair Encoding. We began by examining basic tokenisation approaches, such as splitting text into words or characters, and discussed the limitations of character-level encoding, particularly the inefficiency caused by long input sequences. We then introduced BPE, which builds a vocabulary of frequently occurring subword units, allowing for more efficient and flexible text representation. The notebook demonstrated how BPE leverages repetitive patterns in language and handles rare or out-of-vocabulary words by breaking them into known subwords. We also implemented a simple BPE tokeniser, compared its output to byte- and word-based approaches, and reviewed the vocabulary sizes of popular LLMs. Finally, we introduced OpenAI's `tiktoken` library as a high-performance, production-ready solution for BPE tokenisation. Overall, this notebook provided both theoretical background and practical examples to illustrate why BPE is the preferred tokenisation method for modern LLMs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
